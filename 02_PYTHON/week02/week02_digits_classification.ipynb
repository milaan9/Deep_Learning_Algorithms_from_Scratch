{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"week02_digits_classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iz75Onm1BmVZ"},"source":["# MNIST digits classification with PyTorch\n","\n","In this programming assignment you will implement your first neural network and train it to classify handwritten digits."]},{"cell_type":"code","metadata":{"id":"CR5ons9CxI26"},"source":["%%bash\n","\n","shred -u setup_colab.py\n","\n","wget https://raw.githubusercontent.com/hse-aml/intro-to-dl-pytorch/main/utils/setup_colab.py -O setup_colab.py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BzrfY5t_xI27"},"source":["import setup_colab\n","\n","setup_colab.setup_week02()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M75gw2yqBmVl"},"source":["import numpy as np\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","import tqdm\n","import itertools\n","import collections\n","from IPython import display\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import DataLoader\n","\n","from torchvision.datasets import MNIST\n","from torchvision.transforms import ToTensor\n","from torchvision.utils import make_grid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjtpMRyiHdbt"},"source":["#auxiliary stuff\n","class AverageMeter:\n","    \n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LFrkxLQRBmVm"},"source":["### Fill in your Coursera token and email\n","To successfully submit your answers to our grader, please fill in your Coursera submission token and email."]},{"cell_type":"code","metadata":{"id":"Yobu6DjTBmVn"},"source":["import grading\n","\n","grader = grading.Grader(\n","    assignment_key=\"jNcGh-dHRvuN45xP616Dyw\",\n","    all_parts=[\"zGwHg\", \"5Ww9B\"]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9Q5SMbUBmVn"},"source":["# token expires every 30 min\n","COURSERA_TOKEN = \"### YOUR TOKEN HERE ###\"\n","COURSERA_EMAIL = \"### YOUR EMAIL HERE ###\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SRIiXmfFBmVn"},"source":["## MNIST dataset\n","\n","In this task we will work with MNIST dataset which contains 60000 28x28 images of handwritten digits from 0 to 9. \n","\n","For the data processing we'll use `torchvision` library. It is very simple and easy to use library for computer vision & deep learning. For a deep dive into the library you can check out the githab page: https://github.com/pytorch/vision\n","\n","Firstly, we set up datasets and dataloaders:"]},{"cell_type":"code","metadata":{"id":"q-7Ut2_iBmVo"},"source":["# use it to conver from PIL to torch.Tensor\n","image_transform = ToTensor()\n","\n","train_dataset = MNIST(root='./', train=True, download=True, transform=image_transform)\n","test_dataset = MNIST(root='./', train=False, download=True, transform=image_transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-uuRj-5nBmVp"},"source":["BATCH_SIZE = 32\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_zQK0y3OBmVp"},"source":["Let's look at a batch of images:"]},{"cell_type":"code","metadata":{"id":"Ndo1muoOBmVp"},"source":["example_batch = list(itertools.islice(train_dataloader, 1))[0]\n","images, labels = example_batch\n","\n","# make a grid of images\n","grid_images = make_grid(images, 8).permute(1, 2, 0)\n","\n","print(\"Labels of images: \", labels.view(-1, 8).tolist())\n","\n","plt.figure(figsize=(20, 10))\n","plt.imshow(make_grid(images, 8).permute(1, 2, 0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IUoHQGAnBmVq"},"source":["The task is to train a model that will be able to take an image as an input and predict the class label for it (from 0 to 9) as an output.\n","\n","As a main metric we will use accuracy:"]},{"cell_type":"code","metadata":{"id":"rn8vmPkfBmVr"},"source":["def calculate_accuracy(prediction, target):\n","    # Note that prediction.shape == target.shape == [B, ]\n","    \n","    matching = (prediction == target).float()\n","    return matching.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUqclU3HBmVr"},"source":["## Linear Model\n","\n","Let's start with a linear model and implement it as a neural network in PyTorch. \n","\n","Linear model takes a batch of $B$ images as an input, applies a linear transformation to them and outputs logits. For each image $x$ it returns a vector of logits $z$:\n","$$z = x \\cdot W + b $$\n","\n","Here x.shape = [1, 28 * 28], z.shape = [1, 10].\n","\n","Then, we can make the prediction $\\hat{y}$ for $x$ by taking the class with the maximum logit or obtain probabilities $p$ for all classes by applying softmax function to vector of logits:\n","$$\\hat{y} = \\arg\\max_k z_k \\quad p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad k = 0..9$$\n"]},{"cell_type":"code","metadata":{"id":"cr-sj3X9BmVr"},"source":["# Any neural network in PyTorch is a class with trainable (i.e. requires_grad=True) parameters.\n","# For more detailed tutorial look at https://pytorch.org/tutorials/beginner/nn_tutorial.html\n","\n","class LinearModel(nn.Module):  # inheritance from nn.Module is required\n","    \n","    def __init__(self, input_dim: int, output_dim: int):\n","        super().__init__()  # don't forget to init subclass\n","        \n","        # initialize weight and bias\n","        # NOTE that using of nn.Parameter is required\n","        # Don't use just torch.Tensor\n","        self.weight = nn.Parameter(torch.randn(output_dim, input_dim))\n","        self.bias = nn.Parameter(torch.zeros(output_dim))\n","        \n","        # initialize weight correctly\n","        self.reset_parameters()\n","    \n","    def reset_parameters(self):\n","        nn.init.kaiming_normal_(self.weight)\n","    \n","    def forward(self, input: torch.Tensor):\n","        # We expect input.shape == [B, 1, 28, 28] and need to output logits.shape = [B,10]\n","        \n","        ### YOUR SOLUTION ###\n","        \n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNZ_jPKixqhs"},"source":["INPUT_DIM = 28 * 28\n","OUTPUT_DIM = 10  # num classes\n","\n","linear_model = LinearModel(INPUT_DIM, OUTPUT_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kefMlVIMyCzn"},"source":["# validate shapes\n","\n","assert linear_model.weight.shape == (OUTPUT_DIM, INPUT_DIM)\n","assert linear_model.bias.shape == (OUTPUT_DIM, )\n","assert linear_model.forward(example_batch[0]).shape == (BATCH_SIZE, 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"88KjRwzwyQQr"},"source":["To train the model we will need a loss and an optimizer. \n","\n","We will use a cross-entropy loss, for one object it looks as follows:\n","$$\\text{cross-entropy}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n","\n","where $y$ is a true label, $p_k$ is a predicted probability for class $k$, and  \n","$$\n","[x]=\\begin{cases}\n","       1, \\quad \\text{if $x$ is true} \\\\\n","       0, \\quad \\text{otherwise}\n","    \\end{cases}\n","$$\n","\n","Cross-entropy minimization pushes $p_k$ close to 1 when $y = k$, which is what we want.\n","\n","CrossEntropyLoss criterion in PyTorch combines softmax and loss calculation."]},{"cell_type":"code","metadata":{"id":"tSYCvwVRBmVs"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(linear_model.parameters(), lr=1e-2, momentum=0.9, nesterov=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NtvIcXmRBmVu"},"source":["Write training and testing loops"]},{"cell_type":"code","metadata":{"id":"9x8uy-8dBmVu"},"source":["NUM_EPOCH = 10\n","DEVICE = torch.device('cpu')  # you can change to `cuda:0`\n","HISTORY = collections.defaultdict(list)\n","\n","linear_model.to(DEVICE)\n","\n","for epoch in range(NUM_EPOCH):\n","    # AverageMeter will accumulate average of some metric\n","    # Procceed to `utils.py` to see implementation\n","    train_loss_meter = AverageMeter()\n","    train_accuracy_meter = AverageMeter()\n","    test_loss_meter = AverageMeter()\n","    test_accuracy_meter = AverageMeter()\n","    \n","    # training loop\n","    for train_batch in train_dataloader:\n","        \n","        # unpack batch and move to specific device (for example, GPU or TPU)\n","        images, labels = train_batch\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","        \n","        ### YOUR SOLUTION ###\n","        # do forward pass\n","        # calculate loss (CrossEntropy)\n","        # zero out the previous gradients of our model parameters\n","        # calculate new gradients\n","        # do optimization step\n","        \n","        # calculate current average loss and accuracy\n","        train_loss_meter.update(loss.item())\n","        train_accuracy_meter.update(\n","            calculate_accuracy(\n","                prediction.detach(),\n","                labels\n","            ).item()\n","        )\n","        \n","    # save average train loss and accuracy\n","    HISTORY['train_loss'].append(train_loss_meter.avg)\n","    HISTORY['train_accuracy'].append(train_accuracy_meter.avg)\n","        \n","    # testing loop\n","    for test_batch in test_dataloader:\n","        images, labels = test_batch\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","        \n","        # Ð°dd `with torch.no_grad()' to avoid computing gradients of weights\n","        with torch.no_grad():\n","            # do everything like we did in training loop\n","            logits = linear_model(images)\n","            prediction = logits.argmax(dim=-1)\n","            loss = criterion(logits, labels)\n","        \n","        test_loss_meter.update(loss.item())\n","        test_accuracy_meter.update(\n","            calculate_accuracy(\n","                prediction,\n","                labels\n","            ).item()\n","        )\n","    \n","    # save average test accuracy loss and accuracy\n","    HISTORY['test_loss'].append(test_loss_meter.avg)\n","    HISTORY['test_accuracy'].append(test_accuracy_meter.avg)\n","    \n","    # visualize all together\n","    display.clear_output()\n","    fig, axes = plt.subplots(1, 2, figsize=(20, 7))\n","    axes[0].set_title('Loss (Cross Entropy)')\n","    axes[0].plot(HISTORY['train_loss'], label='Train Loss')\n","    axes[0].plot(HISTORY['test_loss'], label='Test Loss')\n","    axes[0].grid()\n","    axes[0].legend(fontsize=20)\n","    \n","    axes[1].set_title('Accuracy')\n","    axes[1].plot(HISTORY['train_accuracy'], label='Train Accuracy')\n","    axes[1].plot(HISTORY['test_accuracy'], label='Test Accuracy')\n","    axes[1].grid()\n","    axes[1].legend(fontsize=20)\n","    \n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVK59LTcBmVw"},"source":["assert HISTORY['test_accuracy'][-1] > 0.92"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vS9LhPeMxI3B"},"source":["ans_part1 = HISTORY['test_accuracy'][-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6bMyMCKBmVx"},"source":["## GRADED PART, DO NOT CHANGE!\n","grader.set_answer(\"zGwHg\", ans_part1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wj2EH27KxI3C"},"source":["# you can make submission with answers so far to check yourself at this stage\n","grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AhNvJjljBmVx"},"source":["## MLP with hidden layers\n","\n","Previously we've coded a fully-connected linear layer with matrix multiplication by hand. But usually people code only very specific layers by hand, all standard layers are already implemented in PyTorch. The analog of our LinearModel in PyTorch is nn.Linear\n","\n","Now define an MLP with 2 hidden layers. \n","- Do not forget to use nonlinearities between linear layers, for example, nn.ReLU\n","- nn.Sequential help you to combine several layers into one model "]},{"cell_type":"code","metadata":{"id":"AwlSa-JDBmVy"},"source":["INPUT_DIM = 28 * 28\n","OUTPUT_DIM = 10  # num classes\n","\n","# HINT\n","# Use nn.Sequential, nn.Linear and nn.ReLU\n","\n","### YOUR SOLUTION ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"grSmgqzJBmVy"},"source":["Use the code from above to train the model. You're aiming for ~0.97 test accuracy here."]},{"cell_type":"code","metadata":{"id":"MKEDLGhwBmVy","scrolled":false},"source":["HISTORY = collections.defaultdict(list)\n","\n","### YOUR SOLUTION ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iO9wWf-0BmVz"},"source":["ans_part2 = HISTORY['test_accuracy'][-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fm-wxw3yxI3D"},"source":["## GRADED PART, DO NOT CHANGE!\n","grader.set_answer(\"5Ww9B\", ans_part2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6uaveQVWxI3D"},"source":["grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"],"execution_count":null,"outputs":[]}]}