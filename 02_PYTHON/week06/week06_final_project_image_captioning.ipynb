{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYPAWyXv-QKs"
   },
   "source": [
    "# Image Captioning Final Project\n",
    "\n",
    "In this final project you will define and train an image-to-caption model, that can produce descriptions for real world images!\n",
    "\n",
    "<img src=\"images/encoder_decoder.png\" style=\"width:70%\">\n",
    "\n",
    "Model architecture: CNN encoder and RNN decoder. \n",
    "(https://research.googleblog.com/2014/11/a-picture-is-worth-thousand-coherent.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qirwbzmn-QK5"
   },
   "source": [
    "Alright, here's our plan:\n",
    "* Take a pre-trained inception v3 to vectorize images\n",
    "* Stack an LSTM on top of it\n",
    "* Train the thing on MSCOCO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing stuff and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "shred -u setup_colab.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/hse-aml/intro-to-dl-pytorch/main/utils/setup_colab.py -O setup_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_colab\n",
    "\n",
    "setup_colab.setup_week06()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grading\n",
    "import grading_utils\n",
    "\n",
    "grader = grading.Grader(\n",
    "    assignment_key=\"NEDBg6CgEee8nQ6uE8a7OA\",\n",
    "    all_parts=[\"19Wpv\", \"E2OIL\", \"rbpnH\", \"YJR7z\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"### YOUR TOKEN HERE\"\n",
    "COURSERA_EMAIL = \"### YOUR EMAIL HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1613315463748,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "x_wH1bYu-QK8",
    "outputId": "f7c4eafc-3c70-4f3e-d83b-b6ade7f38712"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load dataset (vectorized images and captions)\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "img_codes = np.load(\"data/image_codes.npy\")\n",
    "captions = json.load(open('data/captions_tokenized.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 636,
     "status": "ok",
     "timestamp": 1613315464076,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "VR8TcbYN-QK-",
    "outputId": "5406c930-ef51-4abe-ceab-b219e3a99e38"
   },
   "outputs": [],
   "source": [
    "print(\"Each image code is a 2048-unit vector [ shape: %s ]\" % str(img_codes.shape))\n",
    "print(img_codes[0,:10], end='\\n\\n')\n",
    "print(\"For each image there are 5 reference captions, e.g.:\\n\")\n",
    "print('\\n'.join(captions[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKiDzdJ0-QK-"
   },
   "source": [
    "As you can see, all captions are already tokenized and lowercased. We now want to split them and add some special tokens for start/end of caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo6T5a0O-QK_"
   },
   "outputs": [],
   "source": [
    "# split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"] + sentence.split(' ') + [\"#END#\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz_1Nbm4-QK_"
   },
   "source": [
    "You don't want your network to predict a million-size vector of probabilities at each step, so we're gotta make some cuts.\n",
    "\n",
    "We want you to count the occurences of each word so that we can decide which words to keep in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZF_TT8zs-QK_"
   },
   "outputs": [],
   "source": [
    "# Build a Vocabulary\n",
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "\n",
    "# Compute word frequencies for each word in captions. See code above for data structure\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qdU79Qk-QLA"
   },
   "outputs": [],
   "source": [
    "vocab  = ['#UNK#', '#START#', '#END#', '#PAD#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5 if k not in vocab]\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "assert 10000 <= n_tokens <= 10500\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"19Wpv\", grading_utils.test_vocab(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EADUtf5A-QLA"
   },
   "outputs": [],
   "source": [
    "eos_ix = word_to_index['#END#']\n",
    "unk_ix = word_to_index['#UNK#']\n",
    "pad_ix = word_to_index['#PAD#']\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "ok",
     "timestamp": 1613315600593,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "YDtJMwcl-QLA",
    "outputId": "7e894941-743f-4027-f553-6081a9fe5495"
   },
   "outputs": [],
   "source": [
    "# Try it out on several descriptions of a random image\n",
    "as_matrix(captions[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnzoqZIf-QLB"
   },
   "source": [
    "## Building our neural network\n",
    "As we mentioned earlier, we shall build an rnn \"language-model\" conditioned on vectors from the convolutional part.\n",
    "\n",
    "<img src=\"https://github.com/yunjey/pytorch-tutorial/raw/master/tutorials/03-advanced/image_captioning/png/model.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCHp6va--QLB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBKB5DPR-QLB"
   },
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_tokens=n_tokens, emb_size=128, lstm_units=256, cnn_feature_size=2048):\n",
    "        \"\"\" A recurrent 'head' network for image captioning. See scheme above. \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # a layer that converts conv features to initial_h (h_0) and initial_c (c_0)\n",
    "        self.cnn_to_h0 = nn.Linear(cnn_feature_size, lstm_units)\n",
    "        self.cnn_to_c0 = nn.Linear(cnn_feature_size, lstm_units)\n",
    "\n",
    "        # create embedding for input words. Use the parameters (e.g. emb_size).\n",
    "        # YOUR CODE HERE\n",
    "            \n",
    "        # lstm: create a recurrent core of your network. Use either LSTMCell or just LSTM. \n",
    "        # In the latter case (nn.LSTM), make sure batch_first=True\n",
    "        # YOUR CODE HERE\n",
    "            \n",
    "        # create logits: linear layer that takes lstm hidden state as input and computes one number per token\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, image_vectors, captions_ix):\n",
    "        \"\"\" \n",
    "        Apply the network in training mode. \n",
    "        :param image_vectors: torch tensor containing inception vectors. shape: [batch, cnn_feature_size]\n",
    "        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "            padded with pad_ix\n",
    "        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n",
    "        \"\"\"\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        initial_cell = self.cnn_to_c0(image_vectors)\n",
    "        initial_hid = self.cnn_to_h0(image_vectors)\n",
    "        \n",
    "        # compute embeddings for captions_ix\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # apply recurrent layer to captions_emb. \n",
    "        # 1. initialize lstm state with initial_* from above\n",
    "        # 2. feed it with captions. Mind the dimension order in docstring\n",
    "        # 3. compute logits for next token probabilities\n",
    "        # Note: if you used nn.LSTM, you can just give it (initial_cell[None], initial_hid[None]) as second arg\n",
    "\n",
    "        # lstm_out should be lstm hidden state sequence of shape [batch, caption_length, lstm_units]\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # compute logits from lstm_out\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oBfMNW9-QLC"
   },
   "outputs": [],
   "source": [
    "network = CaptionNet(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1613316480087,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "XfXlXLiV-QLC",
    "outputId": "e7ea62cb-95ed-4efe-ecd7-8e6eedf0f1a2"
   },
   "outputs": [],
   "source": [
    "dummy_img_vec = torch.randn(len(captions[0]), 2048)\n",
    "dummy_capt_ix = torch.tensor(as_matrix(captions[0]), dtype=torch.int64)\n",
    "\n",
    "dummy_logits = network.forward(dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "print('shape:', dummy_logits.shape)\n",
    "assert dummy_logits.shape == (dummy_capt_ix.shape[0], dummy_capt_ix.shape[1], n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yx68R6Fn-QLD"
   },
   "outputs": [],
   "source": [
    "def compute_loss(network, image_vectors, captions_ix):\n",
    "    \"\"\"\n",
    "    :param image_vectors: torch tensor containing inception vectors. shape: [batch, cnn_feature_size]\n",
    "    :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "        padded with pad_ix\n",
    "    :returns: crossentropy (neg llh) loss for next captions_ix given previous ones. Scalar float tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # captions for input - all except last because we don't know next token for last one.\n",
    "    captions_ix_inp = captions_ix[:, :-1].contiguous()\n",
    "    captions_ix_next = captions_ix[:, 1:].contiguous()\n",
    "    \n",
    "    # apply the network, get predictions for captions_ix_next\n",
    "    logits_for_next = network.forward(image_vectors, captions_ix_inp)\n",
    "    \n",
    "    # compute the loss function between logits_for_next and captions_ix_next\n",
    "    # Use the mask!\n",
    "    # Make sure that predicting next tokens after EOS do not contribute to loss\n",
    "    # You can do that either by multiplying elementwise loss by (captions_ix_next != pad_ix)\n",
    "    # or by using ignore_index in some losses.\n",
    "    \n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jDz4fRD-QLD"
   },
   "outputs": [],
   "source": [
    "dummy_loss = compute_loss(network, dummy_img_vec, dummy_capt_ix)\n",
    "\n",
    "assert len(dummy_loss.shape) <= 1, 'loss must be scalar'\n",
    "assert dummy_loss.data.numpy() > 0, \"did you forget the 'negative' part of negative log-likelihood\"\n",
    "\n",
    "dummy_loss.backward()\n",
    "\n",
    "assert all(param.grad is not None for param in network.parameters()), \\\n",
    "    'loss should depend differentiably on all neural network weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"rbpnH\", grading_utils.test_network(network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3IDzYtf-QLD"
   },
   "source": [
    "## Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2094,
     "status": "ok",
     "timestamp": 1613316838905,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "nGojrEFn-QLD",
    "outputId": "6b9269b0-8ed5-44a2-8482-af4b6c5c09cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "captions = np.array(captions)\n",
    "train_img_codes, val_img_codes, train_captions, val_captions = train_test_split(\n",
    "    img_codes, captions, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiYPJTpK-QLE"
   },
   "outputs": [],
   "source": [
    "from random import choice\n",
    "\n",
    "def generate_batch(img_codes, captions, batch_size, max_caption_len=None):\n",
    "    \n",
    "    # sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0, len(img_codes), size=batch_size)\n",
    "    \n",
    "    # get images\n",
    "    batch_images = img_codes[random_image_ix]\n",
    "    \n",
    "    # 5-7 captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    # pick one from a set of captions for each image\n",
    "    batch_captions = list(map(choice,captions_for_batch_images))\n",
    "    \n",
    "    # convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return torch.tensor(batch_images, dtype=torch.float32), \\\n",
    "        torch.tensor(batch_captions_ix, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_batch(img_codes, captions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 834,
     "status": "ok",
     "timestamp": 1613317062173,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "OMetX8Qf-QLE",
    "outputId": "5b94a6a5-8fe4-4f12-d557-90e6df6f78b2"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"E2OIL\", grading_utils.test_batch(generate_batch(img_codes, captions, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klt7Ueuk-QLE"
   },
   "source": [
    "## Train and validate loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqfu7NEyMyev"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FicwmmxaNA4n"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsD-vuuP-QLF"
   },
   "outputs": [],
   "source": [
    "network = CaptionNet(n_tokens).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "n_batches_per_epoch = 50\n",
    "n_validation_batches = 5  # how many batches are used for validation after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189202,
     "status": "ok",
     "timestamp": 1613317618908,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "9NNjNlh8-QLF",
    "outputId": "3e9295a7-0cc6-4ff7-f50a-ede71e33643d"
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    network.train()\n",
    "    for _ in tqdm(range(n_batches_per_epoch)):\n",
    "        images, captions = generate_batch(train_img_codes, train_captions, batch_size)\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "\n",
    "        loss_t = compute_loss(network, images, captions)\n",
    "        \n",
    "        # clear old gradients; do a backward pass to get new gradients; then train with opt\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        train_loss += loss_t.detach().cpu().numpy()\n",
    "        \n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss = 0\n",
    "    network.eval()\n",
    "    for _ in range(n_validation_batches):\n",
    "        images, captions = generate_batch(val_img_codes, val_captions, batch_size)\n",
    "        images = images.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_t = compute_loss(network, images, captions)\n",
    "\n",
    "        val_loss += loss_t.detach().cpu().numpy()\n",
    "\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    clear_output()\n",
    "    print('\\nEpoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"YJR7z\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqCMkyZJ-QLI"
   },
   "source": [
    "## Generating caption\n",
    "\n",
    "The function below creates captions by sampling from probabilities defined by the net.\n",
    "\n",
    "The implementation used here is simple but inefficient (quadratic in lstm steps). We keep it that way since it isn't a performance bottleneck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beheaded_inception3 import beheaded_inception_v3\n",
    "\n",
    "inception = beheaded_inception_v3().eval()\n",
    "\n",
    "def generate_caption(image, caption_prefix = ('#START#',), t=1, sample=True, max_len=100):\n",
    "    network = network.cpu().eval()\n",
    "\n",
    "    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n",
    "           and np.min(image) >= 0 and image.shape[-1] == 3\n",
    "    \n",
    "    image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n",
    "    \n",
    "    vectors_8x8, vectors_neck, logits = inception(image[None])\n",
    "    caption_prefix = list(caption_prefix)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        prefix_ix = as_matrix([caption_prefix])\n",
    "        prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n",
    "        next_word_logits = network.forward(vectors_neck, prefix_ix)[0, -1]\n",
    "        next_word_probs = F.softmax(next_word_logits, -1).detach().numpy()\n",
    "        \n",
    "        assert len(next_word_probs.shape) == 1, 'probs must be one-dimensional'\n",
    "        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab, p=next_word_probs) \n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption_prefix.append(next_word)\n",
    "\n",
    "        if next_word == '#END#':\n",
    "            break\n",
    "\n",
    "    return ' '.join(caption_prefix[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2538,
     "status": "ok",
     "timestamp": 1613319002844,
     "user": {
      "displayName": "Alexander Markovich",
      "photoUrl": "",
      "userId": "05353592946685554048"
     },
     "user_tz": -180
    },
    "id": "tMPXvS5G-QLJ",
    "outputId": "fc28584f-0b58-4904-991b-ab8c8980658c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "# sample image\n",
    "! wget https://avatars.mds.yandex.net/get-zen_doc/1578906/pub_5deb7d24a06eaf00af983448_5deb7d6643863f00ae06dc24/scale_1200 -O data/img.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('data/img.jpg')\n",
    "img = resize(img, (299, 299))\n",
    "\n",
    "plt.imshow(img)\n",
    "\n",
    "for i in range(5):\n",
    "    print(generate_caption(img, t=5.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staff graded assignment tasks\n",
    "\n",
    "Now you are going to train the network and estimate it's quality on checkpoints.\n",
    "\n",
    "1. Train the network for 6 epoches (set n_batches_per_epoch to such value, that with batch_size you iterate over the whole dataset at one epoch, you should increase n_validation_batches to validate more accurately), find good and bad annotation examples. What loss values have you achieved?\n",
    "1. Continue training until 12 epoch. What happened to the loss value? Search for examples, that have improved their captions in comparison to 6 epoch training.\n",
    "1. Train the network until convergence. How many epoches do you need?\n",
    "1. Mobile devices are not very powerfull. Try reducing network size (emb_size=64, lstm_units=128), how it affected network quality after 12 epoches in comparison to original network? And visually?\n",
    "1. Collect at least 10 images that you like to test our network on.\n",
    "    - Seriously, that's part of an assignment. Go get at least 10 pictures to get captioned\n",
    "    - Make sure it works okay on simple images before going to something more complex\n",
    "    - Photos, not animation/3d/drawings, unless you want to train CNN network on anime\n",
    "    - Mind the aspect ratio\n",
    "    - Photos should not be from MSCOCO! Collect some data yourself, take anything you find worth testing on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training tips\n",
    "\n",
    "* If training loss has become close to 0 or model produces garbage, double-check that you're predicting next words, not current or t+2'th words\n",
    "* If the model generates fluent captions that have nothing to do with the images\n",
    "    * this may be due to recurrent net not receiving image vectors.\n",
    "    * alternatively it may be caused by gradient explosion, try clipping 'em or just restarting the training\n",
    "    * finally, you may just need to train the model a bit more\n",
    "* Crossentropy is a poor measure of overfitting\n",
    "    * Model can overfit validation crossentropy but keep improving validation quality.\n",
    "    * Use human (manual) evaluation or try automated metrics: cider or bleu\n",
    "    \n",
    "* We recommend you to periodically evaluate the network using the next \"apply trained model\" block\n",
    "    * its safe to interrupt training, run a few examples and start training again\n",
    "* The typical loss values should be around 3~5 if you average over time, scale by length if you sum over time. The reasonable captions began appearing at loss=2.8 ~ 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to test on\n",
    "\n",
    "We have downloaded 30 images from validation set of MSCOCO, they are located in `data` folder and named `img_i.jpg` for i from 0 to 29. You should use them to measure your network quality during checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ IMAGES FROM DISK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First task\n",
    "\n",
    "Train the network for 6 epoches (set n_batches_per_epoch to such value, that with batch_size you iterate over the whole dataset at one epoch, you should increase n_validation_batches to validate more accurately), find good and bad annotation examples. What loss values have you achieved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE CODE FROM ABOVE (MAIN TRAIN AND VAL LOOP)\n",
    "### USE CODE FROM ABOVE (GENERATING CAPTION FOR IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second task\n",
    "\n",
    "Continue training until 12 epoch. What happened to the loss value? Search for examples, that have improved their captions in comparison to 6 epoch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE CODE FROM ABOVE (MAIN TRAIN AND VAL LOOP)\n",
    "### USE CODE FROM ABOVE (GENERATING CAPTION FOR IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third task\n",
    "\n",
    "Train the network until convergence. How many epoches do you need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE CODE FROM ABOVE (MAIN TRAIN AND VAL LOOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourth task\n",
    "\n",
    " Mobile devices are not very powerfull. Try reducing network size (emb_size=64, lstm_units=128), how it affected network quality after 12 epoches in comparison to original network? And visually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USE CODE FROM ABOVE (MAIN TRAIN AND VAL LOOP)\n",
    "### USE CODE FROM ABOVE (GENERATING CAPTION FOR IMAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth task\n",
    "\n",
    "Collect at least 10 images that you like to test our network on.\n",
    "   - Seriously, that's part of an assignment. Go get at least 10 pictures to get captioned\n",
    "   - Make sure it works okay on simple images before going to something more complex\n",
    "   - Photos, not animation/3d/drawings, unless you want to train CNN network on anime\n",
    "   - Mind the aspect ratio\n",
    "   - Photos should not be from MSCOCO! Collect some data yourself, take anything you find worth testing on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR IMAGES COLLECTING CODE\n",
    "### YOUR CAPTIONS GENERATING CODE"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "week6_final_project_image_captioning_clean.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "157px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03cce6abcd9b4a33b7fad11fab0ea4c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_750308835a8f4073828bc0dae7820810",
      "max": 108857766,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_487601952d6843098f4961e92784bba8",
      "value": 108857766
     }
    },
    "288421d1fc8d4a43ade972d36da57f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "487601952d6843098f4961e92784bba8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "70c5eda26e0a491a80fc2a853fc648db": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74e8b2881eb541d18d028496e3575763": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750308835a8f4073828bc0dae7820810": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebfd785f70d8437ebaaaffb11dfc89bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03cce6abcd9b4a33b7fad11fab0ea4c2",
       "IPY_MODEL_f4d129166ad645318c51c5e26e41c3b0"
      ],
      "layout": "IPY_MODEL_70c5eda26e0a491a80fc2a853fc648db"
     }
    },
    "f4d129166ad645318c51c5e26e41c3b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74e8b2881eb541d18d028496e3575763",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_288421d1fc8d4a43ade972d36da57f45",
      "value": " 104M/104M [00:00&lt;00:00, 114MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}